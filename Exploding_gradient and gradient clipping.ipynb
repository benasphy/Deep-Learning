{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgVirMtGh9JbJPMF7geYVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benasphy/Deep-Learning/blob/main/Exploding_gradient%20and%20gradient%20clipping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kpbE7Y3-owe",
        "outputId": "641a3a53-c0b6-40fc-f713-6b9295ca8c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 - 4s - 122ms/step - loss: nan\n",
            "Epoch 2/10\n",
            "32/32 - 0s - 6ms/step - loss: nan\n",
            "Epoch 3/10\n",
            "32/32 - 0s - 8ms/step - loss: nan\n",
            "Epoch 4/10\n",
            "32/32 - 0s - 9ms/step - loss: nan\n",
            "Epoch 5/10\n",
            "32/32 - 0s - 7ms/step - loss: nan\n",
            "Epoch 6/10\n",
            "32/32 - 0s - 9ms/step - loss: nan\n",
            "Epoch 7/10\n",
            "32/32 - 0s - 10ms/step - loss: nan\n",
            "Epoch 8/10\n",
            "32/32 - 0s - 9ms/step - loss: nan\n",
            "Epoch 9/10\n",
            "32/32 - 0s - 8ms/step - loss: nan\n",
            "Epoch 10/10\n",
            "32/32 - 0s - 9ms/step - loss: nan\n",
            "simple_rnn weights:\n",
            "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            "  nan nan nan nan nan nan nan nan nan nan]]\n",
            "dense weights:\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Generate synthetic data\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "X = np.random.random((1000, 10, 1))  # 1000 samples, 10 timesteps, 1 feature\n",
        "y = np.random.random((1000, 1))      # 1000 samples, 1 target\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(100, activation='tanh', kernel_initializer='random_normal', return_sequences=False),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1.0),  # High learning rate\n",
        "              loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, verbose=2)\n",
        "\n",
        "# Display weights to observe exploding gradients\n",
        "for layer in model.layers:\n",
        "    if hasattr(layer, 'weights'):\n",
        "        print(f\"{layer.name} weights:\")\n",
        "        print(layer.get_weights()[0])  # Kernel weights\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.random((1000, 10, 1))  # 1000 samples, 10 timesteps, 1 feature\n",
        "y = np.random.random((1000, 1))      # 1000 samples, 1 target\n",
        "\n",
        "# Define the RNN model with gradient clipping\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01,  # Reduced learning rate\n",
        "    clipnorm=1.0         # Gradient clipping by norm\n",
        ")\n",
        "\n",
        "model = Sequential([\n",
        "    SimpleRNN(100, activation='tanh',\n",
        "              kernel_initializer='glorot_uniform',  # Xavier initialization\n",
        "              return_sequences=False),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X, y, epochs=10, verbose=2)\n",
        "\n",
        "# Display weights to ensure stability\n",
        "for layer in model.layers:\n",
        "    if hasattr(layer, 'weights'):\n",
        "        print(f\"{layer.name} weights:\")\n",
        "        print(layer.get_weights()[0])  # Kernel weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYCj1oCG__Qb",
        "outputId": "276d97b7-664a-4c08-aa36-21460c3b84f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "32/32 - 2s - 57ms/step - loss: 0.1419\n",
            "Epoch 2/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0964\n",
            "Epoch 3/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0871\n",
            "Epoch 4/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0856\n",
            "Epoch 5/10\n",
            "32/32 - 0s - 5ms/step - loss: 0.0884\n",
            "Epoch 6/10\n",
            "32/32 - 0s - 9ms/step - loss: 0.0858\n",
            "Epoch 7/10\n",
            "32/32 - 0s - 9ms/step - loss: 0.0861\n",
            "Epoch 8/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0854\n",
            "Epoch 9/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0865\n",
            "Epoch 10/10\n",
            "32/32 - 0s - 4ms/step - loss: 0.0839\n",
            "simple_rnn_2 weights:\n",
            "[[-0.0461525   0.21472125 -0.22007725  0.16145101 -0.16700205  0.20489903\n",
            "   0.1090596  -0.19491145 -0.13101982 -0.05267925  0.23131049 -0.1125865\n",
            "   0.15094022 -0.08370709 -0.07294222 -0.14923893  0.16019678  0.13718729\n",
            "   0.06708713 -0.10901699  0.0746131   0.05130262 -0.22834137  0.12415192\n",
            "   0.20925562  0.21970384  0.2237909   0.2094356   0.02910742 -0.05569759\n",
            "   0.1503285   0.0183286   0.00517495 -0.04215881  0.17036273 -0.02344504\n",
            "  -0.08859422 -0.20262705 -0.08082078  0.17524351 -0.0918602   0.20473932\n",
            "  -0.03895865 -0.14550152  0.01323135  0.10823876 -0.1725198   0.03097563\n",
            "   0.01880208  0.06477602 -0.08229557  0.16656749  0.20399126 -0.02148999\n",
            "   0.03765307 -0.13987167 -0.07770874  0.13058658 -0.2293685  -0.07341421\n",
            "   0.07650831  0.19083248  0.07455809  0.19494052  0.0089097  -0.13784504\n",
            "  -0.05540865  0.19638877 -0.13767612  0.11337843  0.1932525   0.08377091\n",
            "  -0.13492051 -0.0868757  -0.01921611 -0.10218181 -0.04750803 -0.21616086\n",
            "  -0.01197091 -0.20697251 -0.22199774  0.01837975  0.10391794 -0.11446764\n",
            "  -0.23539963 -0.06758253  0.18006462  0.23427156  0.09291895 -0.24087892\n",
            "  -0.11238711 -0.03005063  0.06460755 -0.04659894 -0.15921532  0.20893794\n",
            "  -0.2162426  -0.24347925 -0.21691385 -0.24165787]]\n",
            "dense_2 weights:\n",
            "[[ 0.1035246 ]\n",
            " [-0.17205173]\n",
            " [-0.17764242]\n",
            " [ 0.01169992]\n",
            " [-0.18107411]\n",
            " [ 0.12352782]\n",
            " [ 0.13099909]\n",
            " [ 0.14901835]\n",
            " [-0.00415314]\n",
            " [ 0.18202211]\n",
            " [-0.09566271]\n",
            " [-0.1334099 ]\n",
            " [-0.0828269 ]\n",
            " [ 0.21391515]\n",
            " [-0.11479279]\n",
            " [-0.12401164]\n",
            " [ 0.09594607]\n",
            " [-0.07277421]\n",
            " [-0.2050861 ]\n",
            " [ 0.03567758]\n",
            " [-0.15353747]\n",
            " [ 0.00712744]\n",
            " [-0.06234398]\n",
            " [ 0.07493422]\n",
            " [ 0.16401598]\n",
            " [-0.14953344]\n",
            " [-0.0545117 ]\n",
            " [-0.0020212 ]\n",
            " [-0.06490263]\n",
            " [ 0.18146808]\n",
            " [-0.20446649]\n",
            " [ 0.08794589]\n",
            " [-0.09016047]\n",
            " [-0.15645777]\n",
            " [ 0.00773308]\n",
            " [-0.14503847]\n",
            " [ 0.14705053]\n",
            " [ 0.07069089]\n",
            " [-0.02986115]\n",
            " [ 0.13913158]\n",
            " [-0.23555967]\n",
            " [ 0.11180162]\n",
            " [-0.03449187]\n",
            " [ 0.01970085]\n",
            " [ 0.09301794]\n",
            " [-0.11912847]\n",
            " [ 0.03108636]\n",
            " [ 0.07623423]\n",
            " [-0.12703739]\n",
            " [ 0.1691877 ]\n",
            " [ 0.11210238]\n",
            " [-0.1827796 ]\n",
            " [-0.22062206]\n",
            " [-0.10057981]\n",
            " [-0.0750102 ]\n",
            " [ 0.20154594]\n",
            " [ 0.20582621]\n",
            " [ 0.11701583]\n",
            " [-0.0745751 ]\n",
            " [ 0.19141394]\n",
            " [ 0.19906771]\n",
            " [-0.14477442]\n",
            " [-0.20389651]\n",
            " [ 0.24197492]\n",
            " [-0.03014447]\n",
            " [-0.12393416]\n",
            " [ 0.08682305]\n",
            " [-0.15981002]\n",
            " [-0.19175713]\n",
            " [ 0.12860337]\n",
            " [ 0.01513068]\n",
            " [ 0.12677352]\n",
            " [-0.03313075]\n",
            " [-0.0162033 ]\n",
            " [ 0.1792186 ]\n",
            " [ 0.13583769]\n",
            " [-0.18041283]\n",
            " [ 0.01147901]\n",
            " [-0.20112558]\n",
            " [ 0.18714118]\n",
            " [-0.16576965]\n",
            " [-0.0338785 ]\n",
            " [ 0.09586276]\n",
            " [ 0.12998468]\n",
            " [-0.24499689]\n",
            " [ 0.07566337]\n",
            " [-0.04650946]\n",
            " [-0.18398272]\n",
            " [-0.13497803]\n",
            " [-0.21828896]\n",
            " [ 0.20200987]\n",
            " [ 0.07366277]\n",
            " [ 0.17556198]\n",
            " [ 0.1993211 ]\n",
            " [-0.22318375]\n",
            " [-0.05173434]\n",
            " [ 0.10704359]\n",
            " [-0.13707949]\n",
            " [ 0.01566458]\n",
            " [-0.2133218 ]]\n"
          ]
        }
      ]
    }
  ]
}